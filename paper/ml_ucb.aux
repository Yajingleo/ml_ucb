\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{chu2011contextual}
\citation{valko2013finite}
\citation{srinivas2010gaussian}
\citation{zhou2020neural}
\citation{riquelme2018deep}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Algorithm}{2}{section.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Flowchart of the ML-UCB Algorithm}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:flowchart}{{1}{3}{Flowchart of the ML-UCB Algorithm}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Concentration Inequality and UCB}{3}{section.3}\protected@file@percent }
\newlabel{eq:classical_ucb}{{2}{3}{Concentration Inequality and UCB}{equation.2}{}}
\citation{auer2002finite,cappe2013kullback,bartlett2014bandits}
\@writefile{toc}{\contentsline {section}{\numberline {4}Cumulant Generating Function and $\psi $-UCB}{4}{section.4}\protected@file@percent }
\newlabel{eq:concentration}{{7}{4}{Concentration Inequality}{equation.7}{}}
\newlabel{eq:psi_ucb}{{8}{4}{Cumulant Generating Function and $\psi $-UCB}{equation.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Calibrate Concentration Inequality for ML-model}{5}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Learning Curve Analysis}{5}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}From Learning Curve to Concentration Inequality}{5}{subsection.5.2}\protected@file@percent }
\citation{bartlett2014bandits}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Learning curve analysis: $\log (\text  {MSE})$ vs $\log (n)$, where $n$ is the number of training samples. (a) The full training trajectory shows an initial plateau (cold start) followed by power-law decay. The overall slope is $-0.27$, but this is dominated by the cold-start phase. (b) Focusing on the stable regime (last 20\% of training), we observe a cleaner linear relationship with slope $s \approx 0.97$, indicating $\text  {MSE} = O(n^{-0.97})$ convergence.}}{6}{figure.2}\protected@file@percent }
\newlabel{fig:learning_curve}{{2}{6}{Learning curve analysis: $\log (\text {MSE})$ vs $\log (n)$, where $n$ is the number of training samples. (a) The full training trajectory shows an initial plateau (cold start) followed by power-law decay. The overall slope is $-0.27$, but this is dominated by the cold-start phase. (b) Focusing on the stable regime (last 20\% of training), we observe a cleaner linear relationship with slope $s \approx 0.97$, indicating $\text {MSE} = O(n^{-0.97})$ convergence}{figure.2}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Full training trajectory}}}{6}{figure.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Stable regime (last 20\%)}}}{6}{figure.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Regret Analysis}{6}{section.6}\protected@file@percent }
\newlabel{thm: main}{{6.1}{6}{Regret Analysis}{thm.6.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Bounding Condition (1):}{7}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Bounding Condition (2):}{8}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Bounding Condition (3):}{8}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Regret Bound:}{8}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Experiments}{8}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}SGD on Simulated Dataset}{8}{subsection.7.1}\protected@file@percent }
\citation{chu2011contextual}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Results}{9}{subsection.7.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Performance Comparison: ML-UCB vs LinUCB on Collaborative Filtering}}{10}{table.1}\protected@file@percent }
\newlabel{tab:results}{{1}{10}{Performance Comparison: ML-UCB vs LinUCB on Collaborative Filtering}{table.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Comprehensive comparison of ML-UCB ($s=1.0$, $\alpha =10.0$) vs LinUCB ($\alpha =1.0$ and $\alpha =1.4$) over 33,333 iterations. Top row: (left) Cumulative regret showing ML-UCB's superior performance, (center-left) regret rate $R(t)/t$ decreasing over time, (center-right) final regret comparison, (right) regret difference. Bottom row: (left) optimal selection accuracy, (center-left) training MSE learning curves on log scale, (center-right) smoothed instantaneous regret, (right) summary statistics.}}{10}{figure.3}\protected@file@percent }
\newlabel{fig:comparison}{{3}{10}{Comprehensive comparison of ML-UCB ($s=1.0$, $\alpha =10.0$) vs LinUCB ($\alpha =1.0$ and $\alpha =1.4$) over 33,333 iterations. Top row: (left) Cumulative regret showing ML-UCB's superior performance, (center-left) regret rate $R(t)/t$ decreasing over time, (center-right) final regret comparison, (right) regret difference. Bottom row: (left) optimal selection accuracy, (center-left) training MSE learning curves on log scale, (center-right) smoothed instantaneous regret, (right) summary statistics}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Effect of convergence rate $s$ on ML-UCB performance. We compare three settings: $s=0.272$ (full trajectory slope), $s=0.5$ (conservative), and $s=0.97$ (stable regime). Smaller $s$ values produce larger exploration bonuses, while larger $s$ values enable faster transition to exploitation. All experiments use identical ground truth matrices for fair comparison.}}{11}{figure.4}\protected@file@percent }
\newlabel{fig:convergence_comparison}{{4}{11}{Effect of convergence rate $s$ on ML-UCB performance. We compare three settings: $s=0.272$ (full trajectory slope), $s=0.5$ (conservative), and $s=0.97$ (stable regime). Smaller $s$ values produce larger exploration bonuses, while larger $s$ values enable faster transition to exploitation. All experiments use identical ground truth matrices for fair comparison}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Sensitivity Analysis of Convergence Rate}{12}{subsection.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Ablation discussion}{13}{section.8}\protected@file@percent }
\bibstyle{plainnat}
\bibdata{mybib}
\bibcite{auer2002finite}{{1}{2002}{{Auer et~al.}}{{Auer, Cesa-Bianchi, and Fischer}}}
\bibcite{bartlett2014bandits}{{2}{2014}{{Bartlett}}{{}}}
\bibcite{cappe2013kullback}{{3}{2013}{{Capp{\'e} et~al.}}{{Capp{\'e}, Garivier, Maillard, Munos, and Stoltz}}}
\bibcite{chu2011contextual}{{4}{2011}{{Chu et~al.}}{{Chu, Li, Reyzin, and Schapire}}}
\bibcite{riquelme2018deep}{{5}{2018}{{Riquelme et~al.}}{{Riquelme, Tucker, and Snoek}}}
\bibcite{srinivas2010gaussian}{{6}{2010}{{Srinivas et~al.}}{{Srinivas, Krause, Kakade, and Seeger}}}
\bibcite{valko2013finite}{{7}{2013}{{Valko et~al.}}{{Valko, Kveton, Vala, and Munos}}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Conclusion}{14}{section.9}\protected@file@percent }
\bibcite{zhou2020neural}{{8}{2020}{{Zhou et~al.}}{{Zhou, Li, and Gu}}}
\gdef \@abspage@last{15}
